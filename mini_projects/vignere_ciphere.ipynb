{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-yu9bY9AIce",
        "outputId": "582d8def-3330-43ea-c11f-a211d980778e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Setting up configuration...\n",
            "‚ñ∂Ô∏è Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import string\n",
        "import random\n",
        "import math\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"‚úÖ Setting up configuration...\")\n",
        "\n",
        "VOCAB = string.printable\n",
        "VOCAB_SIZE = len(VOCAB)\n",
        "CHAR_TO_IDX = {ch: i for i, ch in enumerate(VOCAB)}\n",
        "IDX_TO_CHAR = {i: ch for i, ch in enumerate(VOCAB)}\n",
        "\n",
        "MAX_LEN = 256\n",
        "D_MODEL = 512\n",
        "NHEAD = 8\n",
        "NUM_ENCODER_LAYERS = 6\n",
        "NUM_DECODER_LAYERS = 6\n",
        "DIM_FEEDFORWARD = 2048\n",
        "DROPOUT = 0.1\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "STEPS_PER_EPOCH = 500\n",
        "MARGIN = 20.0\n",
        "MODEL_SAVE_PATH = \"energy_transformer_vigenere.pth\"\n",
        "\n",
        "\n",
        "INFERENCE_STEPS = 250\n",
        "INFERENCE_LR = 0.1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‚ñ∂Ô∏è Using device: {device}\")\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"‚ö†Ô∏è WARNING: GPU not found. Training will be extremely slow.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"‚úÖ Defining data generation functions and model architecture...\")\n",
        "\n",
        "def vigenere_encrypt(plaintext, key):\n",
        "    ciphertext = []\n",
        "    for p_char, k_char in zip(plaintext, key):\n",
        "        p_idx = CHAR_TO_IDX.get(p_char, 0)\n",
        "        k_idx = CHAR_TO_IDX.get(k_char, 0)\n",
        "        c_idx = (p_idx + k_idx) % VOCAB_SIZE\n",
        "        ciphertext.append(IDX_TO_CHAR.get(c_idx, ''))\n",
        "    return \"\".join(ciphertext)\n",
        "\n",
        "def vigenere_decrypt(ciphertext, key):\n",
        "    plaintext = []\n",
        "    for c_char, k_char in zip(ciphertext, key):\n",
        "        c_idx = CHAR_TO_IDX.get(c_char, 0)\n",
        "        k_idx = CHAR_TO_IDX.get(k_char, 0)\n",
        "        p_idx = (c_idx - k_idx + VOCAB_SIZE) % VOCAB_SIZE\n",
        "        plaintext.append(IDX_TO_CHAR.get(p_idx, ''))\n",
        "    return \"\".join(plaintext)\n",
        "\n",
        "def generate_random_text(length):\n",
        "    return \"\".join(random.choice(VOCAB) for _ in range(length))\n",
        "\n",
        "def corrupt_plaintext(plaintext, corruption_rate=0.2):\n",
        "    pt_list = list(plaintext)\n",
        "    num_corruptions = int(len(pt_list) * corruption_rate)\n",
        "    indices_to_corrupt = random.sample(range(len(pt_list)), num_corruptions)\n",
        "    for idx in indices_to_corrupt:\n",
        "        pt_list[idx] = random.choice(VOCAB)\n",
        "    return \"\".join(pt_list)\n",
        "\n",
        "def generate_training_sample():\n",
        "    length = random.randint(64, MAX_LEN)\n",
        "    plaintext = generate_random_text(length)\n",
        "    key = generate_random_text(length)\n",
        "    ciphertext = vigenere_encrypt(plaintext, key)\n",
        "\n",
        "    neg_plaintext_corrupted = corrupt_plaintext(plaintext)\n",
        "    wrong_key = generate_random_text(length)\n",
        "    while wrong_key == key:\n",
        "        wrong_key = generate_random_text(length)\n",
        "    neg_plaintext_wrong_key = vigenere_decrypt(ciphertext, wrong_key)\n",
        "\n",
        "    return {\n",
        "        \"ciphertext\": ciphertext,\n",
        "        \"positive_plaintext\": plaintext,\n",
        "        \"negative_plaintext_corrupted\": neg_plaintext_corrupted,\n",
        "        \"negative_plaintext_wrong_key\": neg_plaintext_wrong_key\n",
        "    }\n",
        "\n",
        "def get_batch(batch_size):\n",
        "    while True:\n",
        "        yield [generate_training_sample() for _ in range(batch_size)]\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=DROPOUT, max_len=MAX_LEN):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class EnergyTransformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.d_model = D_MODEL\n",
        "        self.embedding = nn.Embedding(VOCAB_SIZE, D_MODEL)\n",
        "        self.pos_encoder = PositionalEncoding(D_MODEL)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=D_MODEL, nhead=NHEAD, num_encoder_layers=NUM_ENCODER_LAYERS,\n",
        "            num_decoder_layers=NUM_DECODER_LAYERS, dim_feedforward=DIM_FEEDFORWARD,\n",
        "            dropout=DROPOUT, batch_first=True\n",
        "        )\n",
        "        self.energy_head = nn.Linear(D_MODEL, 1)\n",
        "\n",
        "    def forward(self, ciphertext_tokens, plaintext_tokens):\n",
        "        ct_embed = self.pos_encoder(self.embedding(ciphertext_tokens) * math.sqrt(self.d_model))\n",
        "        pt_embed = self.pos_encoder(self.embedding(plaintext_tokens) * math.sqrt(self.d_model))\n",
        "        output = self.transformer(src=ct_embed, tgt=pt_embed)\n",
        "        pooled_output = output.mean(dim=1)\n",
        "        return self.energy_head(pooled_output).squeeze(-1)\n",
        "\n",
        "print(\"‚ñ∂Ô∏è Model and data functions are ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JDp3p2FBWqK",
        "outputId": "54fc0991-66f9-4468-9b6a-a54b4ee42565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Defining data generation functions and model architecture...\n",
            "‚ñ∂Ô∏è Model and data functions are ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"‚úÖ Preparing for training...\")\n",
        "\n",
        "model = EnergyTransformer().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "data_loader = get_batch(BATCH_SIZE)\n",
        "\n",
        "def texts_to_tensors(texts):\n",
        "    tensors = torch.zeros(len(texts), MAX_LEN, dtype=torch.long, device=device)\n",
        "    for i, text in enumerate(texts):\n",
        "        tokenized = [CHAR_TO_IDX.get(c, 0) for c in text]\n",
        "        length = min(len(tokenized), MAX_LEN)\n",
        "        tensors[i, :length] = torch.tensor(tokenized[:length], device=device)\n",
        "    return tensors\n",
        "\n",
        "model.train()\n",
        "print(\"üöÄ Starting training...\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0.0\n",
        "    progress_bar = tqdm(range(STEPS_PER_EPOCH), desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    for step in progress_bar:\n",
        "        batch = next(data_loader)\n",
        "\n",
        "        ct = texts_to_tensors([item['ciphertext'] for item in batch])\n",
        "        pt_pos = texts_to_tensors([item['positive_plaintext'] for item in batch])\n",
        "        pt_neg1 = texts_to_tensors([item['negative_plaintext_corrupted'] for item in batch])\n",
        "        pt_neg2 = texts_to_tensors([item['negative_plaintext_wrong_key'] for item in batch])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        energy_positive = model(ct, pt_pos)\n",
        "        energy_negative1 = model(ct, pt_neg1)\n",
        "        energy_negative2 = model(ct, pt_neg2)\n",
        "\n",
        "        loss_neg1 = torch.relu(MARGIN - energy_negative1)\n",
        "        loss_neg2 = torch.relu(MARGIN - energy_negative2)\n",
        "        loss = energy_positive.mean() + loss_neg1.mean() + loss_neg2.mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\", avg_loss=f\"{total_loss / (step + 1):.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / STEPS_PER_EPOCH\n",
        "    print(f\"‚úÖ Epoch {epoch+1} finished. Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "    print(f\"üíæ Model saved to {MODEL_SAVE_PATH}\")\n",
        "\n",
        "print(\"üéâ Training complete!\")"
      ],
      "metadata": {
        "id": "udiEIc6NBdEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"‚úÖ Preparing for inference...\")\n",
        "\n",
        "inference_model = EnergyTransformer().to(device)\n",
        "try:\n",
        "    inference_model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))\n",
        "    inference_model.eval()\n",
        "    print(\"‚ñ∂Ô∏è Trained model loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: Model file not found at {MODEL_SAVE_PATH}. Please train the model first.\")\n",
        "\n",
        "def text_to_tensor(text):\n",
        "    tokens = [CHAR_TO_IDX.get(c, 0) for c in text]\n",
        "    return torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "def embeddings_to_text(embeddings, model_embedding_layer):\n",
        "    vocab_embeddings = model_embedding_layer.weight.detach()\n",
        "    dist = torch.cdist(embeddings.squeeze(0).detach(), vocab_embeddings, p=2)\n",
        "    closest_indices = torch.argmin(dist, dim=1)\n",
        "    return \"\".join([IDX_TO_CHAR.get(idx.item(), '?') for idx in closest_indices])\n",
        "\n",
        "def crack_cipher(ciphertext, model):\n",
        "    print(f\"\\n--- Cracking Ciphertext ---\")\n",
        "    print(f\"Input ({len(ciphertext)} chars): {ciphertext[:80]}...\")\n",
        "\n",
        "    ct_len = min(len(ciphertext), MAX_LEN)\n",
        "    ciphertext = ciphertext[:ct_len]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ct_tensor = text_to_tensor(ciphertext)\n",
        "        ct_embed = model.pos_encoder(model.embedding(ct_tensor) * math.sqrt(model.d_model))\n",
        "        encoder_output = model.transformer.encoder(ct_embed)\n",
        "\n",
        "    pt_guess_embed = torch.randn(1, ct_len, model.d_model, device=device, requires_grad=True)\n",
        "    optimizer = torch.optim.AdamW([pt_guess_embed], lr=INFERENCE_LR)\n",
        "\n",
        "    for step in tqdm(range(INFERENCE_STEPS), desc=\"üîç Searching for plaintext\"):\n",
        "        optimizer.zero_grad()\n",
        "        pt_guess_with_pos = model.pos_encoder(pt_guess_embed)\n",
        "        decoder_output = model.transformer.decoder(tgt=pt_guess_with_pos, memory=encoder_output)\n",
        "        pooled_output = decoder_output.mean(dim=1)\n",
        "        energy = model.energy_head(pooled_output)\n",
        "        energy.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    final_plaintext = embeddings_to_text(pt_guess_embed, model.embedding)\n",
        "    print(f\"Result ({len(final_plaintext)} chars): {final_plaintext[:80]}...\")\n",
        "    return final_plaintext\n",
        "\n",
        "if 'inference_model' in locals() and isinstance(inference_model, nn.Module):\n",
        "    plaintext_secret = \"This is a final project for my cybersecurity class. The goal is to identify ciphers. We are using an energy-based transformer model to see if it can learn the patterns of a Vigenere cipher.\"\n",
        "    key_secret = generate_random_text(len(plaintext_secret))\n",
        "    ciphertext_to_crack = vigenere_encrypt(plaintext_secret, key_secret)\n",
        "\n",
        "    cracked_text = crack_cipher(ciphertext_to_crack, inference_model)\n",
        "\n",
        "    print(\"\\n--- üìä Evaluation ---\")\n",
        "    print(f\"Original Plaintext : {plaintext_secret}\")\n",
        "    print(f\"Cracked Plaintext  : {cracked_text}\")\n",
        "\n",
        "    correct_chars = sum(1 for a, b in zip(plaintext_secret, cracked_text) if a == b)\n",
        "    accuracy = (correct_chars / len(plaintext_secret)) * 100\n",
        "    print(f\"Character-level Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "PhUtL9sNBet7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}