{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gscrdOZFZd00"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random, string, math, os, re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from typing import Tuple\n",
        "\n",
        "#configuration\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "CHARS = string.ascii_uppercase\n",
        "PAD_LABEL = 26\n",
        "A2I = {c: i for i, c in enumerate(CHARS)}\n",
        "I2A = {i: c for i, c in enumerate(CHARS)}\n",
        "I2A[PAD_LABEL] = \"_\"\n",
        "VOCAB_SIZE = len(CHARS)\n",
        "K_MAX = 16  # max key length\n",
        "\n",
        "# Model and training config\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 128\n",
        "LR = 5e-4\n",
        "NUM_ENCODER_BLOCKS = 6\n",
        "EMBED_DIM = 256\n",
        "NHEAD = 8\n",
        "HIDDEN_DIM = 512\n",
        "N_SAMPLES = 10000 # Training samples\n",
        "N_TEST_CASES = 500 # Evaluation samples\n",
        "CHECKPOINT_DIR = \"checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "print(f\"Device: {DEVICE}\")\n",
        "\n",
        "# ============================================================\n",
        "# ENGLISH CORPUS LOADING\n",
        "# ============================================================\n",
        "def get_english_corpus():\n",
        "    try:\n",
        "        import requests\n",
        "        print(\"üìñ Downloading English corpus...\")\n",
        "        url = \"https://www.gutenberg.org/cache/epub/1661/pg1661.txt\"\n",
        "        text = requests.get(url, timeout=20).text.upper()\n",
        "        text = re.sub(r'[^A-Z ]', ' ', text)\n",
        "        words = text.split()\n",
        "        return ' '.join(words[:500000])\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è Corpus download failed, using built-in fallback.\")\n",
        "        words = ['HELLO', 'WORLD', 'TEXT', 'MODEL', 'ENCRYPTION', 'TRAINING',\n",
        "                 'VIGENERE', 'CIPHER', 'KEY', 'MESSAGE', 'DECRYPTION', 'ENGLISH']\n",
        "        return ' '.join(words * 1000)\n",
        "\n",
        "ENGLISH_CORPUS = get_english_corpus()\n",
        "ENGLISH_WORDS = ENGLISH_CORPUS.split()\n",
        "\n",
        "def random_text(n=50):\n",
        "    \"\"\"Generate random English-like plaintext from real words\"\"\"\n",
        "    return ' '.join(random.choices(ENGLISH_WORDS, k=n))\n",
        "\n",
        "# ============================================================\n",
        "# VIGEN√àRE ENCRYPTION & DATA GENERATION\n",
        "# ============================================================\n",
        "def vigenere_encrypt(plaintext, key):\n",
        "    ciphertext = \"\"\n",
        "    key_indices = [A2I[k] for k in key]\n",
        "    j = 0\n",
        "    for ch in plaintext:\n",
        "        if ch in A2I:\n",
        "            shift = key_indices[j % len(key)]\n",
        "            new_idx = (A2I[ch] + shift) % 26\n",
        "            ciphertext += I2A[new_idx]\n",
        "            j += 1\n",
        "        else:\n",
        "            ciphertext += ch\n",
        "    return ciphertext\n",
        "\n",
        "def generate_dataset(n_samples):\n",
        "    data = []\n",
        "    for _ in range(n_samples):\n",
        "        plain = random_text(random.randint(10, 30))\n",
        "        k_len = random.randint(2, K_MAX)\n",
        "        key = ''.join(random.choices(CHARS, k=k_len))\n",
        "        cipher = vigenere_encrypt(plain, key)\n",
        "        data.append((cipher, key))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# DATASET AND DATALOADER UTILITIES\n",
        "# ============================================================\n",
        "class VigenereDataset(Dataset):\n",
        "    def __init__(self, pairs, max_len=500):\n",
        "        self.pairs = pairs\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cipher, key = self.pairs[idx]\n",
        "        c_enc = [A2I.get(ch, PAD_LABEL) for ch in cipher[:self.max_len]]\n",
        "        k_enc = [A2I.get(ch, PAD_LABEL) for ch in key] + [PAD_LABEL]*(K_MAX-len(key))\n",
        "        k_len = len(key)\n",
        "        return torch.tensor(c_enc), torch.tensor(k_enc), torch.tensor(k_len)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    ciphers, keys, key_lens = zip(*batch)\n",
        "    max_len = max(len(c) for c in ciphers)\n",
        "    ciphers = [F.pad(c, (0, max_len - len(c)), value=PAD_LABEL) for c in ciphers]\n",
        "    return torch.stack(ciphers), torch.stack(keys), torch.tensor(key_lens)\n",
        "\n",
        "# ============================================================\n",
        "# MODEL DEFINITION\n",
        "# ============================================================\n",
        "class KeyRecoveryModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_blocks, nhead):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size+1, embed_dim, padding_idx=PAD_LABEL)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim, nhead=nhead, dim_feedforward=hidden_dim, batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_blocks)\n",
        "        self.fc_len = nn.Linear(embed_dim, K_MAX+1)\n",
        "        self.fc_key = nn.Linear(embed_dim, 27)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        enc = self.encoder(x)\n",
        "        pooled = enc.mean(dim=1)\n",
        "        len_pred = self.fc_len(pooled)\n",
        "        key_preds = self.fc_key(pooled.unsqueeze(1).repeat(1, K_MAX, 1))\n",
        "        return len_pred, key_preds"
      ],
      "metadata": {
        "id": "T-eIXeVKZfaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# TRAINING FUNCTION\n",
        "# ============================================================\n",
        "def train_model():\n",
        "    print(f\"\\n--- Starting Training on {DEVICE} ({N_SAMPLES} samples, {EPOCHS} epochs) ---\")\n",
        "    dataset = generate_dataset(N_SAMPLES)\n",
        "    loader = DataLoader(VigenereDataset(dataset), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    model = KeyRecoveryModel(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_ENCODER_BLOCKS, NHEAD).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=LR, total_steps=len(loader)*EPOCHS)\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for x, yk, yl in tqdm(loader, desc=f\"Epoch {epoch}/{EPOCHS}\"):\n",
        "            x, yk, yl = x.to(DEVICE), yk.to(DEVICE), yl.to(DEVICE)\n",
        "            opt.zero_grad()\n",
        "\n",
        "            pred_len, pred_key = model(x)\n",
        "\n",
        "            loss_len = ce_loss(pred_len, yl)\n",
        "            loss_key = ce_loss(pred_key.view(-1, 27), yk.view(-1))\n",
        "            loss = loss_len + loss_key\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(loader)\n",
        "        print(f\"Epoch {epoch}/{EPOCHS} | Avg Loss={avg_loss:.4f}\")\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            torch.save(model.state_dict(), f\"{CHECKPOINT_DIR}/model_epoch_{epoch}.pth\")\n",
        "\n",
        "    print(\"‚úÖ Training Complete\")\n",
        "    return model\n",
        "\n",
        "# Execute Training\n",
        "trained_model = train_model()"
      ],
      "metadata": {
        "id": "sQimgz7_Zgun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# INFERENCE AND EVALUATION FUNCTIONS\n",
        "# ============================================================\n",
        "def predict_key(model, cipher):\n",
        "    \"\"\"Predicts a single key from a cipher string.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor([A2I.get(ch, PAD_LABEL) for ch in cipher]).unsqueeze(0).to(DEVICE)\n",
        "        pred_len, pred_key = model(x)\n",
        "        key_len = pred_len.argmax(dim=1).item()\n",
        "        key_chars = pred_key.squeeze(0).argmax(dim=1).tolist()[:key_len]\n",
        "        predicted_key = ''.join(I2A[int(i)] for i in key_chars if int(i) != PAD_LABEL)\n",
        "    return predicted_key\n",
        "\n",
        "def evaluate_and_generate_csv(model, n_samples=N_TEST_CASES):\n",
        "    \"\"\"\n",
        "    Evaluates the trained model on a new test dataset and generates a CSV.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Evaluation on {n_samples} Test Cases ---\")\n",
        "\n",
        "    # Generate Test Data and DataLoader\n",
        "    test_dataset_pairs = generate_dataset(n_samples)\n",
        "    test_loader = DataLoader(VigenereDataset(test_dataset_pairs), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    model.eval()\n",
        "    total_samples = 0\n",
        "    correct_key_len_predictions = 0\n",
        "    correct_full_key_predictions = 0\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, yk, yl in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            x, yk, yl = x.to(DEVICE), yk.to(DEVICE), yl.to(DEVICE)\n",
        "            batch_size = x.size(0)\n",
        "            total_samples += batch_size\n",
        "\n",
        "            pred_len, pred_key = model(x)\n",
        "            predicted_len_indices = pred_len.argmax(dim=1).cpu().tolist()\n",
        "            predicted_key_indices = pred_key.argmax(dim=2).cpu().tolist()\n",
        "\n",
        "            true_key_indices = yk.cpu().tolist()\n",
        "            true_len_indices = yl.cpu().tolist()\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                true_len = true_len_indices[i]\n",
        "\n",
        "                # --- Key Length Accuracy ---\n",
        "                pred_len_i = predicted_len_indices[i]\n",
        "                len_is_correct = (pred_len_i == true_len)\n",
        "                correct_key_len_predictions += len_is_correct\n",
        "\n",
        "                # --- Key Content and Full Key Recovery ---\n",
        "                pred_key_str = ''.join(I2A[idx] for idx in predicted_key_indices[i][:pred_len_i])\n",
        "                true_key_str = ''.join(I2A[idx] for idx in true_key_indices[i][:true_len])\n",
        "\n",
        "                full_key_is_correct = len_is_correct and (pred_key_str == true_key_str)\n",
        "                correct_full_key_predictions += full_key_is_correct\n",
        "\n",
        "                # Store result row\n",
        "                results.append({\n",
        "                    'ciphertext': ''.join(I2A[idx] for idx in x[i].cpu().tolist() if idx != PAD_LABEL),\n",
        "                    'predicted_key_length': pred_len_i,\n",
        "                    'correct_key_length': true_len,\n",
        "                    'predicted_key': pred_key_str,\n",
        "                    'correct_key': true_key_str,\n",
        "                    # Optional: Add boolean columns for easy filtering\n",
        "                    'length_match': len_is_correct,\n",
        "                    'key_match': full_key_is_correct\n",
        "                })\n",
        "\n",
        "    # Calculate final metrics\n",
        "    len_accuracy = correct_key_len_predictions / total_samples\n",
        "    key_accuracy = correct_full_key_predictions / total_samples\n",
        "\n",
        "    # Save to CSV\n",
        "    df = pd.DataFrame(results)\n",
        "    filename = \"vigenere_model_evaluation_results.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "\n",
        "    print(\"\\n‚úÖ Evaluation and CSV Generation Complete\")\n",
        "    print(\"---------------------------------\")\n",
        "    print(f\"Results saved to: **{filename}**\")\n",
        "    print(f\"Total Test Samples: {total_samples}\")\n",
        "    print(f\"Key Length Accuracy: {len_accuracy * 100:.2f}%\")\n",
        "    print(f\"Full Key Recovery Accuracy: {key_accuracy * 100:.2f}%\")\n",
        "    print(\"---------------------------------\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# --- EXECUTION ---\n",
        "\n",
        "# 1. Evaluate the model and generate CSV\n",
        "evaluation_df = evaluate_and_generate_csv(trained_model)\n",
        "\n",
        "# 2. Example Test\n",
        "test_cipher = \"KCCPKBGUFDPHQTYAVINRRTMVGRKDNBVFDETDGIL\"\n",
        "key_guess = predict_key(trained_model, test_cipher)\n",
        "print(f\"\\nüîë Prediction for Example Cipher: {test_cipher}\")\n",
        "print(f\"Predicted key: {key_guess}\")"
      ],
      "metadata": {
        "id": "R67KTKhsZiVk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}