{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4cd2883",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fc348d8-4969-478d-8337-9bdc5a2ebd2a"
      },
      "source": [
        "! pip install pycryptodome\n",
        "import os, math, random, time\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from Crypto.Cipher import AES, DES\n",
        "from Crypto.Util.Padding import pad\n",
        "from torch.cuda.amp import GradScaler\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycryptodome\n",
            "Successfully installed pycryptodome-3.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8394461"
      },
      "source": [
        "### Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dac77e1"
      },
      "source": [
        "# settings\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "samples_per_class = 50000\n",
        "n_classes = 4\n",
        "batch_size = 256\n",
        "num_workers = 4\n",
        "hist_bins = 64\n",
        "dropout = 0.2\n",
        "\n",
        "# parameters\n",
        "max_len = 512             # Increased from 256 to 512\n",
        "d_model = 256\n",
        "nlayers = 6               # Increased from 4 to 6\n",
        "nhead = 8\n",
        "\n",
        "#Using Semi-Static Data Generation\n",
        "num_parts = 4\n",
        "epochs_per_part = 25\n",
        "num_epochs = num_parts * epochs_per_part\n",
        "\n",
        "#Patience settings\n",
        "patience = 10\n",
        "scheduler_patience = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c49763c6"
      },
      "source": [
        "### Cipher Implementations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3954aaf0"
      },
      "source": [
        "# cipher implementations\n",
        "def random_plaintext(min_len=128, max_len=256):\n",
        "    return os.urandom(random.randint(min_len, max_len))\n",
        "\n",
        "def random_key_for(cipher_name):\n",
        "    if cipher_name == 'AES': return os.urandom(16)\n",
        "    if cipher_name == 'DES': return os.urandom(8)\n",
        "    if cipher_name == 'Speck': return os.urandom(8)\n",
        "    if cipher_name == 'Vigenere': return bytes(random.randint(1,255) for _ in range(random.randint(3,12)))\n",
        "\n",
        "def aes_encrypt(pt, key):\n",
        "    cipher = AES.new(key[:16].ljust(16, b'\\x00'), AES.MODE_CBC)\n",
        "    return cipher.encrypt(pad(pt, AES.block_size))\n",
        "\n",
        "def des_encrypt(pt, key):\n",
        "    cipher = DES.new(key[:8].ljust(8, b'\\x00'), DES.MODE_CBC)\n",
        "    return cipher.encrypt(pad(pt, DES.block_size))\n",
        "\n",
        "def speck_encrypt_block(plain_block, key_words, rounds=22, word_bits=16):\n",
        "    mask = (1 << word_bits) - 1\n",
        "    x, y = plain_block\n",
        "    for k in key_words:\n",
        "        x = (((x >> 8) | ((x << (word_bits - 8)) & mask)) + y) & mask\n",
        "        x ^= k\n",
        "        y = (((y << 3) & mask) | (y >> (word_bits - 3))) ^ x\n",
        "    return x, y\n",
        "\n",
        "def speck_encrypt(pt, key, block_size=4):\n",
        "    out = bytearray()\n",
        "    kw = []\n",
        "    for i in range(0, len(key), 2):\n",
        "        w = (key[i] << 8) | (key[i+1] if i+1 < len(key) else 0)\n",
        "        kw.append(w & 0xFFFF)\n",
        "    pad_len = (-len(pt)) % block_size\n",
        "    pt = pt + bytes([0]*pad_len)\n",
        "    for i in range(0, len(pt), block_size):\n",
        "        a = pt[i:i+block_size//2]; b = pt[i+block_size//2:i+block_size]\n",
        "        x = int.from_bytes(a, 'big'); y = int.from_bytes(b, 'big')\n",
        "        x2, y2 = speck_encrypt_block((x, y), kw)\n",
        "        out.extend(x2.to_bytes(block_size//2, 'big')); out.extend(y2.to_bytes(block_size//2, 'big'))\n",
        "    return bytes(out)\n",
        "\n",
        "def vigenere_encrypt(pt, key):\n",
        "    return bytes((b + key[i % len(key)]) & 0xFF for i, b in enumerate(pt))\n",
        "\n",
        "CIPHER_NAMES = ['AES','DES','Speck','Vigenere']\n",
        "\n",
        "def generate_example_by_name(name):\n",
        "    pt = random_plaintext()\n",
        "    key = random_key_for(name)\n",
        "    if name == 'AES': return aes_encrypt(pt, key)\n",
        "    elif name == 'DES': return des_encrypt(pt, key)\n",
        "    elif name == 'Speck': return speck_encrypt(pt, key, block_size=4)\n",
        "    elif name == 'Vigenere': return vigenere_encrypt(pt, key)\n",
        "    else: raise ValueError(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "350498c2"
      },
      "source": [
        "### Dataset Generation and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a33ca486"
      },
      "source": [
        "# dataset generation\n",
        "def build_dataset_part(samples_per_class, max_len, hist_bins, filename):\n",
        "    if os.path.exists(filename):\n",
        "        print(f\"Loading existing dataset from {filename}\")\n",
        "        data = torch.load(filename)\n",
        "        return data[\"X\"], data[\"y\"], data[\"H\"]\n",
        "\n",
        "    X_list, y_list, H_list = [], [], []\n",
        "    for label, name in enumerate(CIPHER_NAMES):\n",
        "        print(f\"Generating {samples_per_class} samples for {name} for file {filename}...\")\n",
        "        for _ in tqdm(range(samples_per_class)):\n",
        "            ct = generate_example_by_name(name)\n",
        "            if len(ct) > max_len:\n",
        "                ct = ct[:max_len]\n",
        "            else:\n",
        "                ct = ct + bytes([0]*(max_len - len(ct)))\n",
        "            arr = np.frombuffer(ct, dtype=np.uint8)\n",
        "            #histogram feature to helpwith identifying Vignere\n",
        "            factor = 256 // hist_bins\n",
        "            hist = np.zeros(hist_bins, dtype=np.float32)\n",
        "            for b in range(hist_bins):\n",
        "                start, end = b*factor, (b+1)*factor\n",
        "                hist[b] = np.sum((arr >= start) & (arr < end))\n",
        "            hist /= (hist.sum() + 1e-12)\n",
        "            X_list.append(arr)\n",
        "            y_list.append(label)\n",
        "            H_list.append(hist)\n",
        "\n",
        "    X = torch.tensor(np.stack(X_list), dtype=torch.long)\n",
        "    y = torch.tensor(np.array(y_list), dtype=torch.long)\n",
        "    H = torch.tensor(np.stack(H_list), dtype=torch.float32)\n",
        "    torch.save({\"X\": X, \"y\": y, \"H\": H}, filename)\n",
        "    print(\"Saved dataset to\", filename)\n",
        "    return X, y, H\n",
        "\n",
        "# PyTorch Dataset and Dataloader Creation\n",
        "class CipherDataset(Dataset):\n",
        "    def __init__(self, X, y, H):\n",
        "        self.X, self.y, self.H = X, y, H\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx], self.H[idx]\n",
        "\n",
        "def create_dataloaders(X, y, H, batch_size, num_workers):\n",
        "    dataset = CipherDataset(X, y, H)\n",
        "    n_total = len(dataset)\n",
        "    n_train = int(0.8 * n_total)\n",
        "    n_val = int(0.1 * n_total)\n",
        "    n_test = n_total - n_train - n_val\n",
        "\n",
        "    train_ds, val_ds, test_ds = torch.utils.data.random_split(dataset, [n_train, n_val, n_test])\n",
        "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    return train_dl, val_dl, test_dl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af30c7a1"
      },
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d92a4469"
      },
      "source": [
        "#model\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class EnergyTransformer(nn.Module):\n",
        "    def __init__(self, n_tokens=256, d_model=256, nhead=8, nlayers=6, dim_feedforward=512, n_classes=4, label_emb_dim=64, hist_dim=64, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(n_tokens, d_model)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1,d_model))\n",
        "        self.pos = PositionalEncoding(d_model, max_len+1)\n",
        "        layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, activation='gelu', dropout=dropout, batch_first=True)\n",
        "        self.trans = nn.TransformerEncoder(layer, nlayers)\n",
        "        self.label_emb = nn.Embedding(n_classes, label_emb_dim)\n",
        "        input_dim = d_model + label_emb_dim + hist_dim\n",
        "        self.energy_net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "    def forward(self, x, hist):\n",
        "        B = x.size(0)\n",
        "        x = self.embed(x)\n",
        "        cls = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls, x], dim=1)\n",
        "        x = self.pos(x)\n",
        "        x = self.trans(x)\n",
        "        cls_rep = x[:, 0, :]\n",
        "        labels = self.label_emb(torch.arange(self.label_emb.num_embeddings, device=cls_rep.device))\n",
        "        rep_exp = cls_rep.unsqueeze(1).expand(-1, labels.size(0), -1)\n",
        "        lab_exp = labels.unsqueeze(0).expand(B, -1, -1)\n",
        "        hist_exp = hist.unsqueeze(1).expand(-1, labels.size(0), -1)\n",
        "        cat = torch.cat([rep_exp, lab_exp, hist_exp], dim=-1)\n",
        "        energies = self.energy_net(cat).squeeze(-1)\n",
        "        return energies\n",
        "    def predict_logits(self, x, hist):\n",
        "        return -self.forward(x, hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94025a5c"
      },
      "source": [
        "### Training Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62dc5760"
      },
      "source": [
        "# Training helpers\n",
        "def train_epoch(model, dl, opt, scaler, device):\n",
        "    model.train()\n",
        "    total, correct, loss_sum = 0, 0, 0\n",
        "    for xb, yb, hist in tqdm(dl, desc=\"Training\"):\n",
        "        xb, yb, hist = xb.to(device), yb.to(device), hist.to(device)\n",
        "        opt.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits = model.predict_logits(xb, hist)\n",
        "            loss = F.cross_entropy(logits, yb)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "        loss_sum += loss.item() * xb.size(0)\n",
        "        correct += (logits.argmax(1) == yb).sum().item()\n",
        "        total += xb.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model(model, dl, device):\n",
        "    model.eval()\n",
        "    total, correct, loss_sum = 0, 0, 0\n",
        "    for xb, yb, hist in dl:\n",
        "        xb, yb, hist = xb.to(device), yb.to(device), hist.to(device)\n",
        "        logits = model.predict_logits(xb, hist)\n",
        "        loss = F.cross_entropy(logits, yb)\n",
        "        loss_sum += loss.item() * xb.size(0)\n",
        "        correct += (logits.argmax(1) == yb).sum().item()\n",
        "        total += xb.size(0)\n",
        "    return loss_sum/total, correct/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcf4c9a9"
      },
      "source": [
        "### Main Training Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f4c3dfb"
      },
      "source": [
        "# Main Training Execution\n",
        "model = EnergyTransformer(\n",
        "    n_tokens=257, # 256 for bytes + 1 for padding if you use it\n",
        "    d_model=d_model, nhead=nhead, nlayers=nlayers,\n",
        "    dim_feedforward=512, n_classes=n_classes, label_emb_dim=64,\n",
        "    hist_dim=hist_bins, dropout=dropout\n",
        ").to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='max', factor=0.5, patience=scheduler_patience, verbose=True)\n",
        "scaler = GradScaler()\n",
        "\n",
        "best_val = 0\n",
        "patience_counter = 0\n",
        "train_losses, val_accs = [], []\n",
        "global_epoch = 0\n",
        "stop_training = False\n",
        "\n",
        "for part in range(1, num_parts + 1):\n",
        "    if stop_training:\n",
        "        break\n",
        "\n",
        "    print(f\"\\n{'='*20} TRAINING PART {part}/{num_parts} {'='*20}\")\n",
        "\n",
        "    dataset_file = f\"cipher_dataset_{max_len}len_part_{part}.pt\"\n",
        "    X, y, H = build_dataset_part(samples_per_class, max_len, hist_bins, dataset_file)\n",
        "    train_dl, val_dl, test_dl = create_dataloaders(X, y, H, batch_size, num_workers)\n",
        "\n",
        "    for epoch in range(1, epochs_per_part + 1):\n",
        "        global_epoch += 1\n",
        "        print(f\"\\n--- Part {part}, Epoch {epoch}/{epochs_per_part} (Global Epoch {global_epoch}) ---\")\n",
        "\n",
        "        tr_loss, tr_acc = train_epoch(model, train_dl, opt, scaler, device)\n",
        "        val_loss, val_acc = eval_model(model, val_dl, device)\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        train_losses.append(tr_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print(f\"Global Epoch {global_epoch:02d} | Train Loss {tr_loss:.4f} | Train Acc {tr_acc:.3f} | Val Acc {val_acc:.3f} | Best {best_val:.3f}\")\n",
        "\n",
        "        if val_acc > best_val:\n",
        "            best_val = val_acc\n",
        "            torch.save(model.state_dict(), \"best_energy_transformer.pt\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered after {global_epoch} epochs.\")\n",
        "                stop_training = True\n",
        "                break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "701199ba"
      },
      "source": [
        "### Final Test and Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e91bdc8"
      },
      "source": [
        "# -----------------------\n",
        "# Final Test + Plot\n",
        "# -----------------------\n",
        "print(\"\\n--- Final Evaluation on Test Set ---\")\n",
        "model.load_state_dict(torch.load(\"best_energy_transformer.pt\"))\n",
        "test_loss, test_acc = eval_model(model, test_dl, device)\n",
        "print(f\"Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Plotting Training Loss\n",
        "fig1, ax1 = plt.subplots(figsize=(10,6))\n",
        "ax1.set_xlabel(\"Global Epoch\")\n",
        "ax1.set_ylabel(\"Train Loss\", color='tab:red')\n",
        "ax1.plot(range(1, global_epoch + 1), train_losses, color='tab:red', label='Train Loss')\n",
        "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
        "ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "ax1.set_title(\"Training Loss over Global Epochs\")\n",
        "fig1.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plotting Validation Accuracy\n",
        "fig2, ax2 = plt.subplots(figsize=(10,6))\n",
        "ax2.set_xlabel(\"Global Epoch\")\n",
        "ax2.set_ylabel(\"Val Accuracy\", color='tab:blue')\n",
        "ax2.plot(range(1, global_epoch + 1), val_accs, color='tab:blue', label='Val Accuracy')\n",
        "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
        "ax2.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "ax2.set_title(\"Validation Accuracy over Global Epochs\")\n",
        "fig2.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}